{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "\n",
    "class EcommerceETL:\n",
    "    def __init__(self, raw_data_path, processed_data_path):\n",
    "        \"\"\"Initialize ETL pipeline with data paths.\"\"\"\n",
    "        self.raw_data_path = raw_data_path\n",
    "        self.processed_data_path = processed_data_path\n",
    "\n",
    "    def load_event_chunks(self):\n",
    "        \"\"\"Load and concatenate event chunks.\"\"\"\n",
    "        chunk_path = os.path.join(self.raw_data_path, \"events_chunks\")\n",
    "        all_chunks = []\n",
    "        \n",
    "        for file in os.listdir(chunk_path):\n",
    "            if file.endswith(\".csv\"):\n",
    "                chunk = pd.read_csv(os.path.join(chunk_path, file))\n",
    "                all_chunks.append(chunk)\n",
    "        \n",
    "        if not all_chunks:\n",
    "            raise FileNotFoundError(\"No event chunk files found.\")\n",
    "        \n",
    "        # Concatenate all chunks\n",
    "        self.events = pd.concat(all_chunks, ignore_index=True)\n",
    "        print(f\"Loaded {len(self.events)} rows of event data from chunks.\")\n",
    "\n",
    "    def re_chunk_events(self, num_chunks=4):\n",
    "        os.makedirs(self.processed_data_path, exist_ok=True)\n",
    "        chunk_size = len(self.events) // num_chunks\n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = len(self.events) if i == num_chunks - 1 else (i + 1) * chunk_size\n",
    "            chunk = self.events.iloc[start_idx:end_idx]\n",
    "            chunk.to_csv(\n",
    "                os.path.join(self.processed_data_path, f\"temp_events_1_{i + 1}.csv\"),\n",
    "                index=False,\n",
    "            )\n",
    "        logging.info(f\"Re-chunked events data into {num_chunks} files.\")\n",
    "\n",
    "        \n",
    "    def load_raw_data(self):\n",
    "        \"\"\"Load all raw CSV files into dataframes.\"\"\"\n",
    "        self.distribution_centers = pd.read_csv(f\"{self.raw_data_path}/distribution_centers.csv\")\n",
    "        self.load_event_chunks()\n",
    "        self.inventory_items = pd.read_csv(f\"{self.raw_data_path}/inventory_items.csv\")\n",
    "        self.order_items = pd.read_csv(f\"{self.raw_data_path}/order_items.csv\")\n",
    "        self.orders = pd.read_csv(f\"{self.raw_data_path}/orders.csv\")\n",
    "        self.products = pd.read_csv(f\"{self.raw_data_path}/products.csv\")\n",
    "        self.users = pd.read_csv(f\"{self.raw_data_path}/users.csv\")\n",
    "        \n",
    "    def clean_timestamps(self, df, timestamp_columns):\n",
    "        \"\"\"Convert timestamp strings to datetime objects and handle invalid dates.\"\"\"\n",
    "        for col in timestamp_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        return df\n",
    "    \n",
    "    def clean_numeric_values(self, df, numeric_columns):\n",
    "        \"\"\"Clean numeric columns: handle negatives, nulls, and invalid values.\"\"\"\n",
    "        for col in numeric_columns:\n",
    "            if col in df.columns:\n",
    "                # Replace negative values with NaN\n",
    "                df.loc[df[col] < 0, col] = np.nan\n",
    "                # Fill NaN with median for numeric columns\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "        return df\n",
    "    \n",
    "    def clean_categorical_values(self, df, categorical_columns):\n",
    "        \"\"\"Clean categorical columns: standardize case, remove extra spaces.\"\"\"\n",
    "        for col in categorical_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "                df[col] = df[col].replace('nan', np.nan)\n",
    "                df[col] = df[col].fillna('unknown')\n",
    "        return df\n",
    "    \n",
    "    def clean_distribution_centers(self):\n",
    "        \"\"\"Clean distribution centers data.\"\"\"\n",
    "        df = self.distribution_centers.copy()\n",
    "        numeric_cols = ['latitude', 'longitude']\n",
    "        categorical_cols = ['name']\n",
    "        \n",
    "        df = self.clean_numeric_values(df, numeric_cols)\n",
    "        df = self.clean_categorical_values(df, categorical_cols)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def clean_events(self):\n",
    "        \"\"\"Clean events data.\"\"\"\n",
    "        df = self.events.copy()\n",
    "        timestamp_cols = [\"created_at\"]\n",
    "        categorical_cols = [\n",
    "            \"city\", \"state\", \"browser\", \"traffic_source\", \"uri\", \"event_type\"\n",
    "        ]\n",
    "\n",
    "        # Clean timestamps\n",
    "        for col in timestamp_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "        # Clean categorical values\n",
    "        for col in categorical_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "                df[col] = df[col].replace(\"nan\", np.nan)\n",
    "                df[col] = df[col].fillna(\"unknown\")\n",
    "\n",
    "        # Clean IP addresses\n",
    "        if \"ip_address\" in df.columns:\n",
    "            df[\"ip_address\"] = df[\"ip_address\"].fillna(\"0.0.0.0\")\n",
    "\n",
    "        self.events = df\n",
    "        print(\"Cleaned events data.\")\n",
    "\n",
    "    \n",
    "    \n",
    "    def clean_inventory_items(self):\n",
    "        \"\"\"Clean inventory items data.\"\"\"\n",
    "        df = self.inventory_items.copy()\n",
    "        timestamp_cols = ['created_at', 'sold_at']\n",
    "        numeric_cols = ['cost', 'product_retail_price']\n",
    "        categorical_cols = ['product_category', 'product_name', 'product_brand', \n",
    "                          'product_department', 'product_sku']\n",
    "        \n",
    "        df = self.clean_timestamps(df, timestamp_cols)\n",
    "        df = self.clean_numeric_values(df, numeric_cols)\n",
    "        df = self.clean_categorical_values(df, categorical_cols)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def clean_orders_and_items(self):\n",
    "        \"\"\"Clean orders and order items data.\"\"\"\n",
    "        orders_df = self.orders.copy()\n",
    "        items_df = self.order_items.copy()\n",
    "        \n",
    "        timestamp_cols = ['created_at', 'shipped_at', 'delivered_at', 'returned_at']\n",
    "        categorical_cols = ['status', 'gender']\n",
    "        numeric_cols = ['num_of_item']\n",
    "        \n",
    "        # Clean orders\n",
    "        orders_df = self.clean_timestamps(orders_df, timestamp_cols)\n",
    "        orders_df = self.clean_categorical_values(orders_df, categorical_cols)\n",
    "        orders_df = self.clean_numeric_values(orders_df, numeric_cols)\n",
    "        \n",
    "        # Clean order items\n",
    "        items_df = self.clean_timestamps(items_df, timestamp_cols)\n",
    "        items_df = self.clean_categorical_values(items_df, ['status'])\n",
    "        \n",
    "        return orders_df, items_df\n",
    "    \n",
    "    def clean_products(self):\n",
    "        \"\"\"Clean products data.\"\"\"\n",
    "        df = self.products.copy()\n",
    "        numeric_cols = ['cost', 'retail_price']\n",
    "        categorical_cols = ['category', 'name', 'brand', 'department', 'sku']\n",
    "        \n",
    "        df = self.clean_numeric_values(df, numeric_cols)\n",
    "        df = self.clean_categorical_values(df, categorical_cols)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def clean_users(self):\n",
    "        \"\"\"Clean users data.\"\"\"\n",
    "        df = self.users.copy()\n",
    "        timestamp_cols = ['created_at']\n",
    "        numeric_cols = ['age', 'latitude', 'longitude']\n",
    "        categorical_cols = ['first_name', 'last_name', 'gender', 'state', \n",
    "                          'city', 'country', 'traffic_source']\n",
    "        \n",
    "        df = self.clean_timestamps(df, timestamp_cols)\n",
    "        df = self.clean_numeric_values(df, numeric_cols)\n",
    "        df = self.clean_categorical_values(df, categorical_cols)\n",
    "        \n",
    "        # Clean email addresses\n",
    "        df['email'] = df['email'].str.lower().fillna('unknown@example.com')\n",
    "        \n",
    "        # Clean age data\n",
    "        df.loc[df['age'] > 100, 'age'] = df['age'].median()\n",
    "        df.loc[df['age'] < 18, 'age'] = df['age'].median()\n",
    "        \n",
    "        return df\n",
    "    def process_all_data(self):\n",
    "        \"\"\"Process all datasets and save to processed directory.\"\"\"\n",
    "        self.load_raw_data()\n",
    "\n",
    "    # Load and clean event chunks\n",
    "        self.load_event_chunks()\n",
    "        self.clean_events()\n",
    "\n",
    "    # Save cleaned event data\n",
    "        self.events.to_csv(f\"{self.processed_data_path}/cleaned_events.csv\", index=False)\n",
    "\n",
    "    # Clean other datasets\n",
    "        cleaned_dcs = self.clean_distribution_centers()\n",
    "        cleaned_inventory = self.clean_inventory_items()\n",
    "        cleaned_orders, cleaned_order_items = self.clean_orders_and_items()\n",
    "        cleaned_products = self.clean_products()\n",
    "        cleaned_users = self.clean_users()\n",
    "\n",
    "    # Save cleaned datasets\n",
    "        cleaned_dcs.to_csv(f\"{self.processed_data_path}/cleaned_distribution_centers.csv\", index=False)\n",
    "        cleaned_inventory.to_csv(f\"{self.processed_data_path}/cleaned_inventory_items.csv\", index=False)\n",
    "        cleaned_orders.to_csv(f\"{self.processed_data_path}/cleaned_orders.csv\", index=False)\n",
    "        cleaned_order_items.to_csv(f\"{self.processed_data_path}/cleaned_order_items.csv\", index=False)\n",
    "        cleaned_products.to_csv(f\"{self.processed_data_path}/cleaned_products.csv\", index=False)\n",
    "        cleaned_users.to_csv(f\"{self.processed_data_path}/cleaned_users.csv\", index=False)\n",
    "\n",
    "        print(\"All data has been cleaned and saved to the processed directory.\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2431963 rows of event data from chunks.\n",
      "Loaded 2431963 rows of event data from chunks.\n",
      "Cleaned events data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Morsi Store DZ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data has been cleaned and saved to the processed directory.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and run the ETL pipeline\n",
    "etl = EcommerceETL(\n",
    "    raw_data_path='../../data/raw',\n",
    "    processed_data_path='../../data/processed'\n",
    ")\n",
    "etl.process_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-12 14:48:57,765 - Re-chunked cleaned events data into 4 files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "def re_chunk_cleaned_events(input_path, output_path, num_chunks=4):\n",
    "    \"\"\"Re-chunk cleaned events data into smaller files.\"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Load the cleaned events data\n",
    "    cleaned_events = pd.read_csv(input_path)\n",
    "    chunk_size = len(cleaned_events) // num_chunks\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = len(cleaned_events) if i == num_chunks - 1 else (i + 1) * chunk_size\n",
    "        chunk = cleaned_events.iloc[start_idx:end_idx]\n",
    "        \n",
    "        chunk_filename = f\"cleaned_events_chunk_{i + 1}.csv\"\n",
    "        chunk.to_csv(os.path.join(output_path, chunk_filename), index=False)\n",
    "        \n",
    "    logging.info(f\"Re-chunked cleaned events data into {num_chunks} files.\")\n",
    "\n",
    "input_path = '../../data/processed/cleaned_events.csv'\n",
    "output_path = '../../data/cleaned_events_chunks'\n",
    "re_chunk_cleaned_events(input_path, output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
